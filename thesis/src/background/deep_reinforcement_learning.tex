\subsection{Deep Reinforcement Learning}
The advancements in artificial neural networks and deep learning have also spawned a new era of reinforcement learning algorithms that use neural networks as function approximators.

The first published \textit{Deep Reinforcement Learning} algorithm was \textit{Deep Q-learning} (\textit{DQN}) \cite{dqn}, a variation of the Q-learning algorithm that uses an artificial neural network called \textit{Q-network} instead of a Q-table. It also introduced \textit{Experience Replay}, a system in which past experience of an agent is stored in a \textit{replay buffer} so as to make the training process more robust. DQN achieved state-of-the-art, super-human performance in several video games from the \textit{Atari 2600 console}, receiving only the very high dimensional visual information from the game screen as observations.

Over the years, several improvements have been made to the original DQN. For instance, \textit{Double DQN} \cite{double-dqn} uses two neural networks to mitigate DQN's tendency to overestimate action-values. \textit{Prioritized Experience Replay} (\textit{PER}) \cite{per} weighs experience samples from the replay buffer differently based on how good the model already is at correctly estimating their values. For example, an experience sample that is hard for the neural network to learn is chosen more often for training. The \textit{Rainbow} algorithm \cite{rainbow} combines a variety of individual improvements to DQN and manages to significantly outperform each individual improvement.

Actor-critic architectures have also been enhanced with deep neural networks. Notably, the \textit{asynchronous advantage actor-critic} (\textit{A3C}) \cite{a3c} achieved state-of-the-art results even when compared to DQN and can handle continuous state and action spaces. It works by letting several copies of the agent interact with seperate environment instances asynchronously, and combining the gathered experience for training. Similarly to DQN, A3C has received several modifications over the years. For example, the \textit{actor-critic with experience replay} (\textit{ACER}) \cite{acer} improves the sample efficiency of A3C by introducing Experience Replay to the otherwise on-policy algorithm.

Recently, \textit{OpenAI Five} \cite{openai-five} has beaten the world champions at the complex, team-based strategy game \textit{Dota 2}, demonstrating that deep reinforcement learning is even applicable to environments with huge action spaces and very long time horizons. \textit{AlphaStar} \cite{alphastar} has reached grandmaster level performance in the real-time strategy game \textit{StarCraft 2}, which was said to be a new challenge for reinforcement learning \cite{sc2ai}. This was done by training many different versions of the agent in a league system to make each individual agent robust to different strategies. In the field of robotics, a deep reinforcement learning system has managed to solve a Rubik's Cube\footnote{3D combination puzzle invented by Ern≈ë Rubik} using a robotic hand by training in simulations with varying physical attributes to improve robustness of the real world agent, a process called \textit{Automatic Domain Randomization} \cite{rubiks-cube}.