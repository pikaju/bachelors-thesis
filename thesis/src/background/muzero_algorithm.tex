\newcommand{\policy}{\text{\textbf{p}}}
\newcommand{\svalue}{\mathfrak{v}}

\subsection{MuZero Algorithm}
The MuZero algorithm \cite{muzero} is a model-based reinforcement learning algorithm that builds upon the success of its predecessor, \textit{AlphaZero} \cite{alphazero}. Similarly to other model-based algorithms, MuZero can predict the behavior of its environment to plan ahead and choose the most promising action at each timestep to achieve its goal. In contrast to AlphaZero, it does this using a learned model. As such, it can be applied to environments where the rules are not known ahead of time.

MuZero uses an \textit{internal} (or \textit{embedded}) state representation that is deduced from the environment observation but is not required to have any semantic meaning beyond containing sufficient information to predict rewards and values. Accordingly, it may be infeasible to reconstruct observations from internal states. This gives MuZero an advantage over algorithms akin to what is demonstrated in figure \ref{fig:recursive_model}. For example, consider a robot agent receiving visual information through a camera as its observations. Predicting the future color of each of the potentially millions of pixels would not only be notoriously difficult and slow, but also unnecessary, given that we likely only need some key information, such as the position of an object in the environment. Now consider an approach in which we first extract only key information from the observation that is relevant to our task, and then advance this derived knowledge into the future to make decisions. We can see that the latter approach may be advantageous.

There are three distinct functions (e.g. neural networks) that are used harmoneously for planning. Namely, there is a \textit{representation} function $h_\theta$, a \textit{prediction} function $f_\theta$, as well as a \textit{dynamics} function $g_\theta$, each being parameterized using $\theta$ to allow for adjustment through training. The complete model is called $\mu_\theta$. We will now explain each of the three functions in more detail. Note that we will diverge from our naming conventions to stay consistent with those employed in the MuZero paper.
\begin{itemize}
    \item The representation function $h_\theta$ is a mapping from real observations to internal state representations. For a sequence recorded observations $o_1, ..., o_t$ at timestep $t$, an embedded representation $s^0 = h(o_1, ..., o_t)$ may be produced. As previously mentioned, $s^0$ has no semantic meaning, and typically contains less information. Thus, the function $h_\theta$ is tasked with eliminating unnecessary details from $o_t$, for example by extracing object coordinates from images.

    \item The dynamics function $g_\theta$ tries to mimic the environment by advancing an internal state $s^{k-1}$ at a hypothetical timestep $k-1$ based on a chosen action $a^k$ to predict $r^k, s^k = g_\theta\left(s^{k-1}, a^k\right)$, where $r^k$ and $s^k$ are the reward and internal state at timestep $k$, respectively. This function can be applied recursively, similarly to what is shown in figure \ref{fig:recursive_model}, and acts as the simulating part of the algorithm, estimating what may happen when taking a sequence of actions $a^1, ..., a^k$ in a state $s^0$.

    \item The prediction function $f_\theta$ can be compared to an actor-critic architecture, having both a policy and a value output. For any internal state $s^k$, there shall be a mapping $\policy^k, v^k = f_\theta(s^k)$, where $\policy^k$ are the probabilities with which the agent would perform actions and $v^k$ is the future expected reward in state $s^k$. Whereas the value is very useful to bootstrap future rewards after the final step of planning by the dynamics function, the produced policy may be counterintuitive, considering that the agent's policy should be derived from the created plans. We will explore the uses of $\policy^k$ further on.
\end{itemize}

Given parameters $\theta$, we can now decide on an action policy $\pi$ for each observation $o_t$, which we will call the \textit{search policy}, that uses $h_\theta$, $g_\theta$ and $f_\theta$ to search through different action sequences and find an optimal plan. As an example, in a small action space, we can iterate through all action sequences $a_1, ..., a_n$ of a fixed length $n$, and apply each function in the order visualized in figure \ref{fig:muzero_basic_policy}. By discounting the reward and value outputs with a discount factor $\gamma \in [0, 1]$, we receive
\begin{equation*}
    \sum_{k=1}^n \gamma^{k-1} r^k + \gamma^n v^n,
\end{equation*}
which, as we will see, is an estimate for the return when first following the action sequence and subsequently following $\pi$. Given the goal of an agent to maximize the return, we may simply choose the action sequence with the highest return estimate. Alternatively, a less promissing action may be selected to encourage the exploration of new behavior. At timestep $t$, we call the return estimate for the chosen action produced by our search $\svalue_t$.
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[node distance=0.75]
        \node (ot) {$o_t$};
        \node [left = of ot] (otm1) {$o_{t-1}$};
        \node [right = of ot] (otp1) {$o_{t+1}$};
        \node [left = of otm1] (otm2) {$\dots$};
        \node [right = of otp1] (otp2) {$\dots$};
        \draw [->, dashed] (otm2) -- (otm1);
        \draw [->, dashed] (otm1) -- (ot);
        \draw [->, dashed] (ot) -- (otp1);
        \draw [->, dashed] (otp1) -- (otp2);

        \node [below = of ot, draw] (h) {$h_\theta$};
        \node [below = of h] (s0) {$s^0$};
        \draw [->] (ot) -- (h);
        \draw [->] (h) -- (s0);

        \node [right = of s0, draw] (g0) {$g_\theta$};
        \node [right = of g0] (s1) {$s^1$};
        \node [below left = of g0] (a1) {$a^1$};
        \node [above right = of g0] (r1) {$r^1$};
        \draw [->] (s0) -- (g0);
        \draw [->] (a1) edge [bend left] (g0);
        \draw [->] (g0) -- (s1);
        \draw [->] (g0) edge [bend right] (r1);

        \node [right = of s1, draw] (g1) {$g_\theta$};
        \node [right = of g1] (s2) {$s^2$};
        \node [below left = of g1] (a2) {$a^2$};
        \node [above right = of g1] (r2) {$r^2$};
        \draw [->] (s1) -- (g1);
        \draw [->] (a2) edge [bend left] (g1);
        \draw [->] (g1) -- (s2);
        \draw [->] (g1) edge [bend right] (r2);

        \node [right = of s2, draw] (g2) {$g_\theta$};
        \node [right = of g2] (s3) {$s^3$};
        \node [below left = of g2] (a3) {$a^3$};
        \node [above right = of g2] (r3) {$r^3$};
        \draw [->] (s2) -- (g2);
        \draw [->] (a3) edge [bend left] (g2);
        \draw [->] (g2) -- (s3);
        \draw [->] (g2) edge [bend right] (r3);

        \node [below = of s3, draw] (f) {$f_\theta$};
        \node [below = of f] (pv) {$\policy^3, v^3$};
        \draw [->] (s3) -- (f);
        \draw [->] (f) -- (pv);
    \end{tikzpicture}
    \caption{Testing a single action sequence made up of three actions $a^1$, $a^2$ and $a^3$ to plan ahead from observation $o_t$ using all three MuZero functions.}
    \label{fig:muzero_basic_policy}
\end{figure}

Training occurs on previously observed trajectories produced by the MuZero agent that may be stored in a replay buffer. For each timestep $t$ in the trajectory, we unroll our functions $K$ steps through time and adjust $\theta$ such that the predictions further match what was already observed in the trajectory. Each reward output $r^k_t$ of $g_\theta$ is trained on the real reward $u_{t+k}$. We also apply the prediction function $f_\theta$ to each of the internal states $s^0_t, ..., s^K_t$, giving us policy predictions $\policy^0_t, ..., \policy^K_t$ and value predictions $v^0_t, ..., v^K_t$. Each policy prediction $\policy^k_t$ is trained on the stored search policy $\pi_{t+k}$. This makes $\policy$ an estimate (that is faster to compute) of what our search policy $\pi$ might be, meaning it can be used as a heuristic. For our value estimates $\svalue$, we first calculate $n$-step bootstrapped returns $z_t = u_{t+1} + \gamma u_{t+2} + ... + \gamma^{n-1} u_{t+n} + \gamma^n\svalue_{t+n}$. The target $z_{t+k}$ is set for each value output $v^k_t$. The three previously mentioned output targets as well as an additional L2 regularization term $c||\theta||^2$ form the complete loss function:
\begin{equation*}
    l_t(\theta) = \sum^K_{k=0} \left(l^r(u_{t+k}, r_t^k) + l^v(z_{t+k}, v_t^k) + l^p(\pi_{t+k}, \policy_t^k) + c||\theta||^2\right)
\end{equation*}

The aforementioned bruteforce search policy is highly unoptimized. For one, we have not described a method of caching and reusing internal states and prediction function outputs so as to reduce the computational footprint. Furthermore, iterating through all possible actions is very slow for high numbers of actions or longer action sequences, and impossible for continuous action spaces. Ideally, we would want to focus our planning efforts on only those actions that are promising from the beginning, and spend less time incorporating clearly unfavorable behavior. AlphaZero and MuZero employ a variation of \textit{Monte-Carlo tree search} (\textit{MCTS}) \cite{mcts}, which we will elaborate in the following section.

It is advisable to use a replay buffer for MuZero reach its potential with regards to sample efficiency. That is, we store experience gathered in the environment in a buffer for some time and reuse the same experience samples multiple times during training. Because the losses are calculated based on the respective search policy $\pi$ and value $\svalue$, $\pi$ and $\svalue$ must be readily available. Recalculating search results for each training iteration would take a tremendous amount of time and processing power, which is why they are instead stored alongside the actual experience after they are determined the first time. This, in turn, means that the stored policies and values, produced by the parameterized model $\mu_\theta$, become increasingly outdated as the training progresses and $\theta$ is updated. Nevertheless, empirical evidence shows that the algorithm works. However, the problem can be mitigated through an advanced method called \textit{MuZero Reanalyze} \cite{muzero}, in which replay samples are regularly updated by performing the search algorithm with the latest available model parameters. This has been shown to further improve MuZero, but comes at the cost of significantly larger computational requirements.

MuZero matches the state-of-the-art performance of AlphaZero in the board games Chess and \textit{Shogi}, despite not having access to a perfect environment model. It even exceeded AlphaZero's unprecedented performance in the board game \textit{Go}, while using less computation per node, suggesting that it caches useful information in its internal states to gain a deeper understanding of the environment with each application of the dynamics function. Furthermore, MuZero outperforms humans and previous state-of-the-art agents at the Atari game benchmark, demonstrating its ability to solve tasks with long time horizons that are not turn-based.