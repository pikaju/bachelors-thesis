\subsubsection{Policy-Based Methods}
We now explain one possible approach to optimize an agent's behavior by directly manipulating its parameterized policy $\pi_\theta$ with regards to the expected returns $\mathbb{E}\left[G_t\right]$. Typically, this is done using gradient descent. The \textit{REINFORCE} algorithm \cite{reinforce} defines an estimate for $\nabla_\theta \mathbb{E}\left[G_t \given S_t \right]$ as
\begin{equation*}
    \nabla_\theta \mathbb{E}\left[G_t \given S_t \right] \approx \nabla_\theta \log \pi_\theta\left(A_t \given S_t\right) G_t,
\end{equation*}
coining the term \textit{policy gradient}. Intuitively, this may be thought of as increasing or decreasing the probability of actions on a given trajectory based on the return of said trajectory.

Because the return of an entire trajectory needs to be determined before adjusting the policy, REINFORCE may, by itself, only be used in episodic tasks where training occurs at the end of an episode.

The regular is prone to high variance, which can slow down learning. A baseline $b(S_t)$ can be introduced, which is shown to remain unbiased and often improves performance:
\begin{equation*}
    \nabla_\theta \mathbb{E}\left[G_t \given S_t \right] \approx \nabla_\theta \log \pi_\theta\left(A_t \given S_t\right) \left(G_t - b(S_t)\right)
\end{equation*}