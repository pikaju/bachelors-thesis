\subsubsection{Policy-Based Methods}
We now explain one possible approach to optimize an agent's behavior by directly manipulating its parameterized policy $\pi_\theta$ with regards to the expected returns $\mathbb{E}\left[G_t\right]$. Typically, this is done using gradient descent. The \textit{REINFORCE} algorithm \cite{reinforce} defines an estimate for $\nabla_\theta \mathbb{E}\left[G_t \given S_t \right]$ as
\begin{equation*}
    \nabla_\theta \mathbb{E}\left[G_t \given S_t \right] \approx \nabla_\theta \log \pi_\theta\left(A_t \given S_t\right) G_t,
\end{equation*}
coining the term \textit{policy gradient}. Intuitively, this may be thought of as increasing or decreasing the probability of actions on a given trajectory depending on the trajectory's return.

Because the returns $G_t$ cannot be determined before a terminal state has been reached, REINFORCE may, by itself, only be used in episodic tasks where training occurs at the end of an episode.

The regular policy gradient is prone to high variance, which can slow down learning. A baseline $b(S_t)$ can be introduced, which is shown to remain unbiased and often improves performance:
\begin{equation*}
    \nabla_\theta \mathbb{E}\left[G_t \given S_t \right] \approx \nabla_\theta \log \pi_\theta\left(A_t \given S_t\right) \left(G_t - b(S_t)\right)
\end{equation*}