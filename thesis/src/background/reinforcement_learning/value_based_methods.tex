\subsubsection{Value-Based Methods}
In value-based methods, the agent maintains estimates of the state-value or action-value functions to improve its behavior. The key concept of value-based methods is the recursive definition for value functions, known as the \textit{Bellman equation} \cite{bible}:
\begin{align*}
    v_\pi &= \mathbb{E}_\pi \left[G_t \given S_t = s\right] \\
          &= \mathbb{E}_\pi \left[R_t + G_{t+1} \given S_t = s\right] \\
          &= \sum_a \pi\left(a \given s\right) \sum_{s'} \sum_r p\left(s', r \given s, a\right) \left[r + \gamma \mathbb{E}_\pi\left[G_{t+1} \given S_{t+1} = s'\right]\right] \\
          &= \sum_a \pi\left(a \given s\right) \sum_{s'} \sum_r p\left(s', r \given s, a\right) \left[r + \gamma v_\pi\left(s'\right)\right]
\end{align*}
Put into words, the value of a state is the sum of all possible immediate rewards and discounted values of subsequent states, weighted by their occurrence probabilities. A similar definition can be created for action-value functions:
\begin{equation*}
    q_\pi\left(s, a\right) = \sum_{s'} \sum_r p\left(s', r \given s, a\right) \left[r + \gamma q_\pi(s', a')\right]
\end{equation*}
The optimal policy $\pi^*$, given the correct action-value function $q_{\pi^*}$, would always perform the action with the highest $q$-value, i.e. the largest expected return. In turn, the action-values created by this policy are always the highest possible expected returns of any action. We find ourselves in a stable condition, known as the \textit{Bellman optimality equation} \cite{bible}, where $q_*$ denotes the action-value function for the optimal policy:
\begin{equation*}
    q_*\left(s, a\right) = \sum_{s'} \sum_r p\left(s', r \given s, a\right) \left[r + \gamma \max_{a'} q_*(s', a')\right]
\end{equation*}

In practice, the transition probabilities $p$ are usually not known. Q-learning \cite{q-learning} approaches this issue by iteratively updating a $q$-value estimates (which we will refer to as $Q$) in a tabular form, known as the Q-table. The iterative formula
\begin{equation*}
    Q\left(S_t, A_t\right) \gets Q\left(S_t, A_t\right) + \beta\left(R_{t+1} + \gamma \max_a Q\left(S_{t+1}, A_t\right) - Q\left(S_t, A_t\right)\right),
\end{equation*}
where $\beta$ is the learning rate hyperparameter, closely resembles the Bellman optimality equation. We are updating the current value estimate from the $Q$-table entry using the acquired information $R_{t+1}$ as well as a future, \textit{bootstrapped} value estimate $\max_a Q\left(S_{t+1}, A_t\right)$. This form of learning from future estimates is referred to as \textit{temporal-difference learning} or \textit{TD learning} \cite{td-learning}. The difference between the current estimate and the bootstrapped estimated combined with new experience is called the \textit{TD error}.

Note that Q-learning, similarly to other value-based methods, requires discrete, finite actions, whereas the aforementioned REINFORCE algorithm is capable of handling continuous action spaces. To define a Q-table, the number of possible environment states must also be finite and relatively small, which makes visual tasks virtually impossible. However, unlike REINFORCE, Q-learning is an \mbox{\textit{online}} algorithm \cite{bible}, meaning it can learn from data as soon as it is available (while interacting with the environment), and does not need to wait for the end of an episode. Thus, it can also be used for non-terminating tasks.