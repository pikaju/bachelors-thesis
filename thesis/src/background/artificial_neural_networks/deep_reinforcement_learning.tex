\subsubsection{Deep Reinforcement Learning}
The advancements in artificial neural networks and deep learning have also spawned a new era of reinforcement learning algorithms that use neural networks as function approximators.

The first published \textit{Deep Reinforcement Learning} algorithm was \textit{Deep Q-learning} (\textit{DQN}) \cite{dqn}, a variation of the Q-learning algorithm that uses an artificial neural network called \textit{Q-network} instead of a Q-table. It also introduced \textit{Experience Replay}, a system in which past experience of an agent is stored in a \textit{replay buffer} so as to make the training process more robust. DQN achieved state-of-the-art, super-human performance in several video games from the \textit{Atari 2600 console}, receiving only the very high dimensional visual information from the game screen as observations.

Over the years, several improvements have been made to the original DQN. For instance, \textit{Double DQN} \cite{double-dqn} uses two neural networks to mitigate DQN's tendency to overestimate action-values. \textit{Prioritized Experience Replay} (\textit{PER}) \cite{per} weighs experience samples from the replay buffer differently based on how good the model already is at correctly estimating their values. For example, an experience sample that is hard for the neural network to learn is chosen more often for training. The \textit{Rainbow} algorithm \cite{rainbow} combines a variety of individual improvements to DQN and manages to significantly outperform each individual improvement.

Actor-critic architectures have also been enhanced with deep neural networks. Notably, the \textit{asynchronous advantage actor-critic} (\textit{A3C}) \cite{a3c} achieved state-of-the-art results even when compared to DQN and can handle continuous state and action spaces. It works by letting several copies of the agent interact with seperate environment instances asynchronously, and combining the gathered experience for training. Similarly to DQN, A3C has received several modifications over the years. For example, the \textit{actor-critic with experience replay} (\textit{ACER}) \cite{acer} improves the sample efficiency of A3C by introducing Experience Replay to the otherwise on-policy algorithm.