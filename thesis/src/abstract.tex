\begin{abstract}
    \noindent
    Using a model of the environment, reinforcement learning agents can plan ahead and achieve super-human performance in board games like Chess, Shogi and Go, while remaining relatively sample efficient. As demonstrated by the MuZero Algorithm, the environment model may even be learned dynamically, generalizing the agent to many more tasks while achieving state-of-the-art performance. Notably, MuZero uses internal state representations instead of real environment states for its predictions. In this thesis, we introduce two additional, independent loss terms to MuZero's overall loss function, which work entirely unsupervised and act as constraints to stabilize the learning process. Experiments show that they provide a significant performance increase on simple OpenAI Gym environments. The modifications also enable self-supervised pretraining for MuZero, that is, the algorithm can learn about environment dynamics before a goal is made available.
\end{abstract}

\vfill

\renewcommand{\abstractname}{Zusammenfassung}
\begin{abstract}
    \noindent
    Mithilfe eines Modells der Umgebung können Reinforcement Learning Agenten vorausplanen und beispielsweise Menschen in Brettspielen wie Schach, Shōgi und Go schlagen, ohne dafür viele Trainingsbeispiele zu benötigen. Insbesondere hat der MuZero Algorithmus bestätigt, dass das Modell durch Interaktion mit der Umgebung gelernt und somit nicht bereit gestellt werden muss, ohne dabei die State of the Art Leistung zu beeinträchtigen. MuZero verwendet hierfür interne Repräsentationen der Umgebungszustände. In dieser Bachelorthesis beschreiben wir zwei zusätzliche Loss-Terme, welche vollständig unüberwacht funktionieren, und MuZero's Loss-Funktion beigefügt werden können um den Lernprozess zu stabilisieren. Experimentell können wir so eine Verbesserung der Leistung in einfachen OpenAI Gym Umgebungen zeigen. Zusätzlich erlauben die Änderungen, dass der MuZero Agent die Mechaniken seiner Umgebung bereits vor Einführung eines Ziels erlernen kann.
\end{abstract}