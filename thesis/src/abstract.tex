\begin{abstract}
    \noindent
    Using a model of the environment, reinforcement learning agents can plan ahead and achieve super-human performance in board games like Chess, Shogi and Go, while remaining relatively sample efficient. As demonstrated by the MuZero Algorithm, the environment model may even be learned dynamically, generalizing the agent to many more tasks while achieving state-of-the-art performance. Notably, MuZero uses internal state representations instead of real environment states for its predictions. In this thesis, we introduce two additional, independent loss terms to MuZero's overall loss function, which work entirely unsupervised and act as constraints to stabilize the learning process. Experiments show that they provide a significant performance increase on simple OpenAI Gym environments. The modifications also enable self-supervised pretraining for MuZero, that is, the algorithm can learn about environment dynamics before a goal is made available.
\end{abstract}

\vfill

\renewcommand{\abstractname}{Zusammenfassung}
\begin{abstract}
    \noindent
    
\end{abstract}