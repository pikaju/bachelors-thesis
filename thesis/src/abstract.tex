\begin{abstract}
    \noindent
    Using a model of the environment, reinforcement learning agents can plan their future moves and achieve super-human performance in board games like Chess, Shogi, and Go, while remaining relatively sample efficient. As demonstrated by the MuZero Algorithm, the environment model can even be learned dynamically, generalizing the agent to many more tasks while at the same time achieving state-of-the-art performance. Notably, MuZero uses internal state representations instead of real environment states for its predictions. In this thesis, we introduce two additional, independent loss terms to MuZero's overall loss function, which work entirely unsupervised and act as constraints to stabilize the learning process. Experiments show that they provide a significant performance increase in simple \mbox{OpenAI Gym} environments. Our modifications also enable self-supervised pretraining for MuZero, meaning the algorithm can learn about environment dynamics before a goal is made available.
\end{abstract}

\vspace{1cm}

\renewcommand{\abstractname}{Zusammenfassung}
\begin{abstract}
    \noindent
    Mithilfe eines Modells der Umgebung können Reinforcement Learning Agenten vorausplanen, und so beispielsweise Menschen in Brettspielen wie Schach, Shōgi und Go schlagen, ohne dafür viele Trainingsbeispiele zu benötigen. Insbesondere hat der MuZero Algorithmus bestätigt, dass das Modell durch Interaktion mit der Umgebung erlernt werden kann (und somit nicht bereitgestellt werden muss) ohne dabei die herausragenden Ergebnisse zu beeinträchtigen. MuZero verwendet hierfür interne Repräsentationen der Umgebungszustände. In dieser Bachelorthesis beschreiben wir zwei zusätzliche Loss-Terme, welche vollständig unüber\-wacht funktionieren, und MuZeros Loss-Funktion beigefügt werden können um den Lernprozess zu stabilisieren. Experimentell können wir so eine Verbesserung der Leistung in einfachen OpenAI Gym Umgebungen zeigen. Zusätzlich erlauben die Änderungen, dass der MuZero Agent die Mechaniken seiner Umgebung bereits vor Einführung eines Ziels erlernen kann.
\end{abstract}

\vfill

\begin{center}
    Note: Figures that do not include a citation were drawn by the author.
\end{center}