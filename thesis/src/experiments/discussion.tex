\subsection{Discussion}
The results show that all modified agents we tested exceed the performance of the unchanged MuZero agent on all environments, some, especially the agents including both proposed changes simultaneously, by a large amount. The reconstruction function seems to be the more influential of the two changes, given that it achieved a bigger performance increase than the consistency loss, and comes very close to even the hybrid agent. Even so, the consistency loss has the advantage of being simpler in its implementation by not requiring a fourth function, potentially made up of complex neural networks requiring maintenance by experts, to be added.

Agents that were pretrained in a self-supervised fashion ahead of time performed very well in the early stages of training but were unable to outmatch their fully trained counterparts. This is to be expected. While self-supervised pretraining is meant to accelerate learning, there is no argument as to why training with partial information should yield higher maximum scores in the long run. In CartPole-v1, the pretrained agent was even significantly worse than the non-pretrained version after roughly $1000$ training steps. This is likely due to the parameters and internal state representations of the model prematurely converging, to some extent, to a local minimum, which made reward and value estimations more difficult after the goal was introduced. The use of L2 regularization during pretraining may prevent this unwanted convergence.

It is still unclear whether the consistency loss provides additional value to the algorithm that is not reproducible by the reconstruction loss. While the excellent performance of the hybrid agent does suggest a benefit, we may see similar results when further increasing the weight of the reconstruction loss, considering that the agent using the highest tested loss weight performed the best. This would render the hybrid architecture unnecessary. Thus, further investigation regarding hyperparameter choices is required.

While the consistency loss resulted in a performance increase, the experiments were not designed to confirm that state representations did indeed become more consistent. Future work should also validate our hypothesis stating that the consistency loss may lessen the accuracy falloff when planning further than $K$ timesteps into the future, with $K$ being the number of unrolled steps during training.
