\subsection{Discussion}
The results show that all modified agents we tested agents exceeded the performance of the unchanged MuZero agent on all environments, some, especially the agents including both proposed changes simulatenously, by a very large amount. The reconstruction function seems to be the most important of the two changes, given that it achieved a larger performance increase than the consistency loss, and comes very close to even the hybrid agent. Even so, the consistency loss has the advantage of being simpler in its implementation by not requiring a fourth function, potentially made up of complex neural networks requiring maintenance by experts, to be added.

It is still unclear wether the consistency loss provides additional value to the algorithm that is not reproducable by the reconstruction loss. While the excellent performance of the hybrid agent does suggest a benefit, we may see similar results when further increasing the weight of the reconstruction loss, which would render the hybrid architecture unnecessary. Thus, further investigation is required.

Agents that were pretrained in a self-supervised fashion ahead of time performed very well in the early stages of training, but were unable to outperform their fully trained counterparts. This is to be expected. While self-supervised pretraining is supposed to accelerate learning, there is no argument as to why training with partial information should yield higher maximum scores in the long run. In CartPole-v1, the pretrained agent was even significantly worse than the non-pretrained version after roughly $1000$ training steps. This is likely because the parameters and internal state representations prematurely converged, to some extent, to a local minimum, which made reward and value estimations more difficult after the goal was introduced. The use of L2 regularization during pretraining may prevent this unwanted convergence.