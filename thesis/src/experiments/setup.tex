\subsection{Setup}
We adopt \textit{muzero-general} \cite{muzero-general}, an open source implementation of MuZero that is implemented in the Python programming language and uses PyTorch for automatic differentiation, as our baselines agent, and modify the code to include a reconstruction function and additional loss terms. The readout of the replay buffer must also be tweaked to include not only the observation at timestep $t$, but the $K$ subsequent observations $o_{t+1}, ..., o_{t+K}$ as well, as they are critical for calculating our new loss values.

We test a variety of different weights for each of the two loss terms in order to gauge their potential capability of improving performance, both individually and in union. Furthermore, as a means of showcasing self-supervised learning for MuZero, we pretrain a hybrid agent, that is, an agent using both modifications at the same time, for $5000$ training steps, using only the newly added loss terms, instead of the full loss formula.

Table \ref{tab:hyperparameters} shows the hyperparameters used by all tested model configurations. Most of the listed hyperparameters were kept as is from the muzero-general defaults. Note that we are unable to perform a comprehensive hyperparameter search due to technical limitations.
\begin{table}[ht]
    \centering
    \begin{tabular}{|c|l||c|c|}
        \hline
        & & CartPole-v1 & LunarLander-v2 \\
        \hline\hline

        & Training steps & 10000 & 30000 \\
        \cline{2-4}
        & Discount factor ($\gamma$) & 0.997 & 0.999 \\
        \cline{2-4}
        & TD steps ($n$) & 50 & 30 \\
        \cline{2-4}
        & Unroll steps ($K$) & 10 & 10 \\
        \cline{2-4}
        & Internal state dimensions & 10 & 10 \\

        \hline

        \multirow{4.1}{*}{\begin{sideways}Losses\end{sideways}} & Optimizer & Adam & Adam \\
        \cline{2-4}
        & Learning rate ($\beta$) & $0.02 \times 0.9^{t \times 0.001}$ & 0.005 \\
        \cline{2-4}
        & Value loss weight & 1.0 & 1.0 \\
        \cline{2-4}
        & L2 regularization weight & $10^{-4}$ & $10^{-4}$ \\

        \hline

        \multirow{3.2}{*}{\begin{sideways}Replay\end{sideways}} & Replay buffer size & 500 & 2000 \\
        \cline{2-4}
        & Prioritization exponent & 0.5 & 0.5 \\
        \cline{2-4}
        & Batch size & 128 & 64 \\

        \hline

        \multirow{7.5}{*}{\begin{sideways}MCTS\end{sideways}} & Simulations & 50 & 50 \\
        \cline{2-4}
        & Dirichlet $\alpha$ & 0.25 & 0.25 \\
        \cline{2-4}
        & Exploration factor & 0.25 & 0.25 \\
        \cline{2-4}
        & pUCT $c_1$ & 1.25 & 1.25 \\
        \cline{2-4}
        & pUCT $c_2$ & 19652 & 19652 \\
        \cline{2-4}
        & Softmax temperature ($T$) & \makecell{
            1.0 if $t<5000$, \\ 0.5 if $5000 \leq t < 7500$, \\ 0.25 if $t \geq 7500$
        } & 0.35 \\

        \hline
    \end{tabular}
    \caption{Hyperparameter selection for all tested agents. Parameters based on the current training step use the variable $t$.}
    \label{tab:hyperparameters}
\end{table}

 Performance is measured by training an agent for a specific amount of training steps and, at various time steps, sampling the total episode reward the agent can achieve.
