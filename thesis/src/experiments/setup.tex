\subsection{Setup}
We adopt \textit{muzero-general} \cite{muzero-general}, an open source implementation of MuZero that is written in the Python programming language and uses PyTorch for automatic differentiation, as our baselines agent.
\begin{figure}[ht]
    \centering
    \tikzset{%
        cascaded/.style = {%
            general shadow = {%
            shadow scale = 1,
            shadow xshift = -1ex,
            shadow yshift = -1ex,
            draw,
            thick,
            fill = white},
            general shadow = {%
            shadow scale = 1,
            shadow xshift = -.5ex,
            shadow yshift = -.5ex,
            draw,
            thick,
            fill = white},
            fill = white, 
            draw,
            thick,
        }
    }
    \begin{tikzpicture}[every node/.style={inner sep=0.3cm}]
        \node [draw, rectangle] (reanalyze) {Reanalyze};
        \node [draw, rectangle, below = of reanalyze] (replay) {Replay Buffer};
        \node [draw, rectangle, below = 1.5cm of replay] (storage) {Shared Storage};
        \node [draw, rectangle, left = 2.5cm of storage] (trainer) {Trainer};
        \node [draw, rectangle, cascaded, right = 2.5cm of storage] (play) {Self Play};
        \node [draw, rectangle, below = 1.5cm of storage] (tensorboard) {TensorBoard};

        \draw [->] (reanalyze) edge [bend right] node[left] {update game history} (replay);
        \draw [->] (replay) edge [bend right] node[right] {sample game history} (reanalyze);

        \draw [->] (trainer) -- node[below right=-0.2cm] {update priorities} (replay);
        \draw [->] (replay) edge [bend right] node[above left] {get batch} (trainer);
        \draw [->] (trainer) -- node[below] {set weights} (storage);
        \draw [->] (storage) -- node[below] {get weights} (play);
        \draw [->] (play) -- node[above right] {save game history} (replay);

        \draw [->] (storage) -- node[left] {get info} (tensorboard);
    \end{tikzpicture}
    \caption{The parallelized worker structure of muzero-general.}
    \label{fig:muzero_general}
\end{figure}
It is heavily parallelized by employing a worker architecture (see figure \ref{fig:muzero_general}), with each worker being a seperate process that communicates with its peers through message passing. This allows for flexible scaling, even across multiple machines. Worker types include
\begin{itemize}
    \item a \textit{Replay Buffer} worker, responsible for storing game histories as experience for the learning process,
    \item a \textit{Shared Storage} worker, which holds logging information and distributes neural network weights,
    \item at least one, but potentially many \textit{Self Play} processes, each interacting with its own instance of the environment, collecting experience and filling the replay buffer,
    \item a \textit{Trainer}, which uses stored experience to perform gradient descent on the loss function, thereby improving the model,
    \item the \textit{Reanalyze} worker, responsible for continuously updating outdated search policies and values within the replay buffer and
    \item the main process, which takes logging information from the Shared Storage and outputs it to \textit{TensorBoard}, a tool that allows for live visualization of the training process.
\end{itemize}
Note that the term \textit{self-play} originates from AlphaZero or MuZero agents playing a board game like Chess against themselves to learn strategies without any human influence. Nevertheless, we use the term even for environments operated by only a single player, such as LunarLander-v2.

We modify the source code of muzero-general to include a reconstruction function and additional loss terms. The readout of the replay buffer must also be tweaked to include not only the observation at timestep $t$, but the $K$ subsequent observations $o_{t+1}, ..., o_{t+K}$ as well, as they are critical for calculating our new loss values.

The neural networks making up each of the four MuZero functions follow a very simple structure in our experiments. They consist of simple two-layer fully connected perceptrons. The first (\textit{hidden}) layer contains 16 neurons in the case of of CartPole-v1, and 64 for LunarLander-v2. The number of neurons in the second layer is determined by the desired output dimensionality. For example, the representation function must output an internal state, which, for CartPole-v1, is an eight-dimensional vector. It is therefore made up of eight neurons.

A variety of different weights are tested for each of the two loss terms in order to gauge their potential capability of improving performance, both individually and in union. Furthermore, as a means of showcasing self-supervised learning for MuZero, we pretrain a hybrid agent, that is, an agent using both modifications at the same time, for $5000$ training steps, using only the newly added loss terms, instead of the full loss formula.

Table \ref{tab:hyperparameters} shows the hyperparameters used by all tested model configurations. Most of the listed hyperparameters were kept as is from the muzero-general defaults. Note that we are unable to perform a comprehensive hyperparameter search due to technical limitations.
\begin{table}[ht]
    \centering
    \begin{tabular}{|c|l||c|c|}
        \hline
        & & CartPole-v1 & LunarLander-v2 \\
        \hline\hline

        & Training steps & 10000 & 30000 \\
        \cline{2-4}
        & Discount factor ($\gamma$) & 0.997 & 0.999 \\
        \cline{2-4}
        & TD steps ($n$) & 50 & 30 \\
        \cline{2-4}
        & Unroll steps ($K$) & 10 & 10 \\
        \cline{2-4}
        & Internal state dimensions & 8 & 10 \\
        \cline{2-4}
        & MuZero Reanalyze & Enabled & Enabled \\

        \hline

        \multirow{4.1}{*}{\begin{sideways}Losses\end{sideways}} & Optimizer & Adam & Adam \\
        \cline{2-4}
        & Learning rate ($\beta$) & $0.02 \times 0.9^{t \times 0.001}$ & 0.005 \\
        \cline{2-4}
        & Value loss weight & 1.0 & 1.0 \\
        \cline{2-4}
        & L2 regularization weight & $10^{-4}$ & $10^{-4}$ \\

        \hline

        \multirow{3.2}{*}{\begin{sideways}Replay\end{sideways}} & Replay buffer size & 500 & 2000 \\
        \cline{2-4}
        & Prioritization exponent & 0.5 & 0.5 \\
        \cline{2-4}
        & Batch size & 128 & 64 \\

        \hline

        \multirow{7.5}{*}{\begin{sideways}MCTS\end{sideways}} & Simulations & 50 & 50 \\
        \cline{2-4}
        & Dirichlet $\alpha$ & 0.25 & 0.25 \\
        \cline{2-4}
        & Exploration factor & 0.25 & 0.25 \\
        \cline{2-4}
        & pUCT $c_1$ & 1.25 & 1.25 \\
        \cline{2-4}
        & pUCT $c_2$ & 19652 & 19652 \\
        \cline{2-4}
        & Softmax temperature ($T$) & \makecell{
            1.0 if $t<5000$, \\ 0.5 if $5000 \leq t < 7500$, \\ 0.25 if $t \geq 7500$
        } & 0.35 \\

        \hline
    \end{tabular}
    \caption{Hyperparameter selection for all tested agents. Parameters based on the current training step use the variable $t$.}
    \label{tab:hyperparameters}
\end{table}

 Performance is measured by training an agent for a specific amount of training steps and, at various time steps, sampling the total episode reward the agent can achieve.
