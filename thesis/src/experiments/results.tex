\subsection{Results}
The following figures show a comparisson of the performance of agents with different weights applied to each individual loss terms proposed in this thesis.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] ]
        \begin{axis}[
            title = CartPole-v1,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/CartPole-v1/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/CartPole-v1/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^g$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^g$};
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] 
        \begin{axis}[
            title = LunarLander-v2,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/LunarLander-v2/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/LunarLander-v2/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^g$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^g$};
        \end{axis}
    \end{tikzpicture}
    \caption{Total episode reward comparisson of agents using the reconstruction loss term ($l^g$) and the default MuZero agent in the CartPole-v1 and LunarLander-v2 environments, averaged across 32 and 25 runs, respectively.}
    \label{fig:reconstruction_results}
\end{figure}
The results in figure \ref{fig:reconstruction_results} show an increase in performance when adding the reconstruction function together with its associated loss term to the MuZero algorithm on all testing environments. Weighing the reconstruction loss term with $\frac{1}{2}$ only has a minor negative impact on the learning process. Note that, in the LunarLander-v2 environment, a penalty reward of $-100$ is given to an agent for crashing the lander. The default MuZero agent was barely able to exceed this threshold, whereas the reconstruction agent achieved positive total rewards.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] ]
        \begin{axis}[
            title = CartPole-v1,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/CartPole-v1/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/consistency/CartPole-v1/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/consistency/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^c$};
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] 
        \begin{axis}[
            title = LunarLander-v2,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/LunarLander-v2/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/consistency/LunarLander-v2/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/consistency/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^c$};
        \end{axis}
    \end{tikzpicture}
    \caption{Total episode reward comparisson of agents the consistency loss ($l^c$) and the default MuZero agent in the CartPole-v1 and LunarLander-v2 environments, averaged across 32 and 25 runs, respectively.}
    \label{fig:consistency_results}
\end{figure}
The consistency loss term agent matched or only very slightly exceeded the performance of MuZero in the CartPole-v1 environment, as can be seen in figure \ref{fig:consistency_results} (left). However, in the LunarLander-v2 task, the modified agent significantly outperformed MuZero, being almost at the same level as the reconstruction agent. A loss weight of $1$ is also notably better than a loss weight of $\frac{1}{2}$.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] ]
        \begin{axis}[
            title = CartPole-v1,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/CartPole-v1/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^g$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/CartPole-v1/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^g + \frac{1}{2}l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^g + l^c$};
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] 
        \begin{axis}[
            title = LunarLander-v2,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/LunarLander-v2/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^g$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/LunarLander-v2/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^g + \frac{1}{2}l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^g + l^c$};
        \end{axis}
    \end{tikzpicture}
    \caption{Total episode reward comparisson of agents using both the reconstruction function loss ($l^g$) as well as the consistency loss term ($l^c$) simultaneously in the CartPole-v1 and LunarLander-v2 environments, averaged across 32 and 25 runs, respectively.}
    \label{fig:hybrid_results}
\end{figure}
An agent using both loss terms simultaneously outperforms MuZero (visible in figure \ref{fig:hybrid_results}), and even scores marginally better than the reconstruction loss agent, in all environments tested.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] ]
        \begin{axis}[
            title = CartPole-v1,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/CartPole-v1/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^g + l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/pretrained/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^g + l^c$ (pretrained)};
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] 
        \begin{axis}[
            title = LunarLander-v2,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/LunarLander-v2/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^g + l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/pretrained/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^g + l^c$ (pretrained)};
        \end{axis}
    \end{tikzpicture}
    \caption{Total episode reward comparisson of agents using both the reconstruction function loss ($l^g$) as well as the consistency loss term ($l^c$) simultaneously as a pretrained and non-pretrained variant in the CartPole-v1 and LunarLander-v2 environments, averaged across 32 and 25 runs, respectively.}
    \label{fig:pretrained_results}
\end{figure}
When using self-supervised pretraining (see figure \ref{fig:pretrained_results}), training progresses very rapidly as soon as the goal is introduced. In the LunarLander-v2 environment, a mean total reward of $0$ is reached in roughly half the amount of training steps that are required by the non-pretrained agent. However, at later stages of training, the advantage fades, and, in the case of CartPole-v1, agents using self-supervised pretraining perform significantly worse than agents starting with randomly initialized networks.