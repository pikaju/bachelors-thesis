\subsection{Results}
The following figures show a comparison of the performance of agents with different weights applied to each loss term proposed in this thesis. For our notation, we use $l^g$ and $l^c$ for the reconstruction function and consistency loss modification, respectively. With $\frac{1}{2}l^g$ and $\frac{1}{2}l^g$ we denote that the default loss weight of $1$ for each term has been changed to $\frac{1}{2}$. Finally, we write a plus sign to indicate the combination of both modifications.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] ]
        \begin{axis}[
            title = CartPole-v1,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
            xmin=0,
            xmax=10000,
            grid=major,
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/CartPole-v1/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/CartPole-v1/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^g$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^g$};
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] 
        \begin{axis}[
            title = LunarLander-v2,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
            xmin=0,
            xmax=30000,
            grid=major,
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/LunarLander-v2/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/LunarLander-v2/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^g$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^g$};
        \end{axis}
    \end{tikzpicture}
    \caption{Total episode reward comparison of agents using the reconstruction loss term ($l^g$) and the default MuZero agent in the CartPole-v1 and LunarLander-v2 environments, averaged across 32 and 25 runs, respectively.}
    \label{fig:reconstruction_results}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] ]
        \begin{axis}[
            title = CartPole-v1,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
            xmin=0,
            xmax=10000,
            grid=major,
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/CartPole-v1/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/consistency/CartPole-v1/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/consistency/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^c$};
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] 
        \begin{axis}[
            title = LunarLander-v2,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
            xmin=0,
            xmax=30000,
            grid=major,
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/LunarLander-v2/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/consistency/LunarLander-v2/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/consistency/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^c$};
        \end{axis}
    \end{tikzpicture}
    \caption{Total episode reward comparison of agents the consistency loss ($l^c$) and the default MuZero agent in the CartPole-v1 and LunarLander-v2 environments, averaged across 32 and 25 runs, respectively.}
    \label{fig:consistency_results}
\end{figure}
The consistency loss term agent matched or only very slightly exceeded the performance of MuZero in the CartPole-v1 environment, as can be seen in figure \ref{fig:consistency_results} (left). However, in the LunarLander-v2 task, the modified agent significantly outperformed MuZero, being almost at the same level as the reconstruction agent. A loss weight of $1$ is also notably better than a loss weight of $\frac{1}{2}$.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] ]
        \begin{axis}[
            title = CartPole-v1,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
            xmin=0,
            xmax=10000,
            grid=major,
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/CartPole-v1/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^g$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/CartPole-v1/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^g + \frac{1}{2}l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^g + l^c$};
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] 
        \begin{axis}[
            title = LunarLander-v2,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
            xmin=0,
            xmax=30000,
            grid=major,
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/LunarLander-v2/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^g$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/LunarLander-v2/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^g + \frac{1}{2}l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^g + l^c$};
        \end{axis}
    \end{tikzpicture}
    \caption{Total episode reward comparison of agents using both the reconstruction function loss ($l^g$) as well as the consistency loss term ($l^c$) simultaneously in the CartPole-v1 and LunarLander-v2 environments, averaged across 32 and 25 runs, respectively.}
    \label{fig:hybrid_results}
\end{figure}
An agent using both loss terms simultaneously outperforms MuZero (visible in figure \ref{fig:hybrid_results}), and even scores marginally better than the reconstruction loss agent, in all environments tested.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] ]
        \begin{axis}[
            title = CartPole-v1,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
            xmin=0,
            xmax=10000,
            grid=major,
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/CartPole-v1/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^g + l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/pretrained/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^g + l^c$ (pretrained)};
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] 
        \begin{axis}[
            title = LunarLander-v2,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
            xmin=0,
            xmax=30000,
            grid=major,
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/LunarLander-v2/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^g + l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/pretrained/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^g + l^c$ (pretrained)};
        \end{axis}
    \end{tikzpicture}
    \caption{Total episode reward comparison of agents using both the reconstruction function loss ($l^g$) as well as the consistency loss term ($l^c$) simultaneously as a pretrained and non-pretrained variant in the CartPole-v1 and LunarLander-v2 environments, averaged across 32 and 25 runs, respectively.}
    \label{fig:pretrained_results}
\end{figure}
When using self-supervised pretraining (see figure \ref{fig:pretrained_results}), training progresses very rapidly as soon as the goal is introduced. In the LunarLander-v2 environment, a mean total reward of $0$ is reached in roughly half the amount of training steps that are required by the non-pretrained agent. However, at later stages of training, the advantage fades, and, in the case of CartPole-v1, agents using self-supervised pretraining perform significantly worse than agents starting with randomly initialized networks.

The fully trained agents are compared in table \ref{tab:results_table}. Training took place on NVIDIA GeForce GTX 1080 and NVIDIA GeForce RTX 2080 Ti GPUs. Each experiment required roughly two to three days to complete.
\begin{table}[ht]
    \centering
    \begin{tabular}{|l||c|c|}
        \hline
        & CartPole-v1 & LunarLander-v2 \\
        \hline \hline
        MuZero & $281.42 \pm 162.48$ & $-34.86 \pm 92.87$ \\
        \hline
        $l^g$ & $375.85 \pm 149.38$ & $42.67 \pm 132.59$\\
        \hline
        $l^c$ & $296.45 \pm 174.55$ & $32.17 \pm 145.90$ \\
        \hline
        $l^g + l^c$ & $\mathbf{410.59 \pm 130.71}$ & $100.46 \pm 123.82$ \\
        \hline
        $l^g + l^c$ (pretrained) & $335.72 \pm 162.49$ & $\mathbf{104.99 \pm 116.67}$ \\
        \hline
    \end{tabular}
    \caption{Comparison of the default MuZero algorithm and the modifications described in this thesis on the CartPole-v1 and LunarLander-v2 environments. The terms $l^g$ and $l^c$ stand for the addition of a reconstruction or consistency loss, respectively. The results show the mean and standard deviation of the total reward for the final $500$ training steps across all test runs.}
    \label{tab:results_table}
\end{table}