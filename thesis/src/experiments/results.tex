\subsection{Results}
The following figures show a comparisson of the performance of agents with different weights applied to each individual loss terms proposed in this thesis.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] ]
        \begin{axis}[
            title = CartPole-v1,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
            xmin=0,
            xmax=10000,
            grid=major,
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/CartPole-v1/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/CartPole-v1/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^g$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^g$};
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] 
        \begin{axis}[
            title = LunarLander-v2,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
            xmin=0,
            xmax=30000,
            grid=major,
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/LunarLander-v2/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/LunarLander-v2/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^g$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^g$};
        \end{axis}
    \end{tikzpicture}
    \caption{Total episode reward comparisson of agents using the reconstruction loss term ($l^g$) and the default MuZero agent in the CartPole-v1 and LunarLander-v2 environments, averaged across 32 and 25 runs, respectively.}
    \label{fig:reconstruction_results}
\end{figure}
The results in figure \ref{fig:reconstruction_results} show an increase in performance when adding the reconstruction function together with its associated loss term to the MuZero algorithm on all testing environments. Weighing the reconstruction loss term with $\frac{1}{2}$ only has a minor negative impact on the learning process. Note that, in the LunarLander-v2 environment, a penalty reward of $-100$ is given to an agent for crashing the lander. The default MuZero agent was barely able to exceed this threshold, whereas the reconstruction agent achieved positive total rewards.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] ]
        \begin{axis}[
            title = CartPole-v1,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
            xmin=0,
            xmax=10000,
            grid=major,
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/CartPole-v1/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/consistency/CartPole-v1/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/consistency/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^c$};
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] 
        \begin{axis}[
            title = LunarLander-v2,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
            xmin=0,
            xmax=30000,
            grid=major,
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/LunarLander-v2/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/consistency/LunarLander-v2/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/consistency/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^c$};
        \end{axis}
    \end{tikzpicture}
    \caption{Total episode reward comparisson of agents the consistency loss ($l^c$) and the default MuZero agent in the CartPole-v1 and LunarLander-v2 environments, averaged across 32 and 25 runs, respectively.}
    \label{fig:consistency_results}
\end{figure}
The consistency loss term agent matched or only very slightly exceeded the performance of MuZero in the CartPole-v1 environment, as can be seen in figure \ref{fig:consistency_results} (left). However, in the LunarLander-v2 task, the modified agent significantly outperformed MuZero, being almost at the same level as the reconstruction agent. A loss weight of $1$ is also notably better than a loss weight of $\frac{1}{2}$.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] ]
        \begin{axis}[
            title = CartPole-v1,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
            xmin=0,
            xmax=10000,
            grid=major,
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/CartPole-v1/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^g$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/CartPole-v1/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^g + \frac{1}{2}l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^g + l^c$};
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] 
        \begin{axis}[
            title = LunarLander-v2,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
            xmin=0,
            xmax=30000,
            grid=major,
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/LunarLander-v2/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/reconstruction/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^g$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/LunarLander-v2/lr0.5.csv};
            \addlegendentry{$\frac{1}{2}l^g + \frac{1}{2}l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^g + l^c$};
        \end{axis}
    \end{tikzpicture}
    \caption{Total episode reward comparisson of agents using both the reconstruction function loss ($l^g$) as well as the consistency loss term ($l^c$) simultaneously in the CartPole-v1 and LunarLander-v2 environments, averaged across 32 and 25 runs, respectively.}
    \label{fig:hybrid_results}
\end{figure}
An agent using both loss terms simultaneously outperforms MuZero (visible in figure \ref{fig:hybrid_results}), and even scores marginally better than the reconstruction loss agent, in all environments tested.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] ]
        \begin{axis}[
            title = CartPole-v1,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
            xmin=0,
            xmax=10000,
            grid=major,
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/CartPole-v1/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^g + l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/pretrained/CartPole-v1/lr1.0.csv};
            \addlegendentry{$l^g + l^c$ (pretrained)};
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[yscale=0.7, xscale=0.7] 
        \begin{axis}[
            title = LunarLander-v2,
            axis lines = left,
            xlabel = Training steps,
            ylabel = Total reward,
            no markers,
            table/col sep = comma,
            legend cell align=left,
            legend pos=south east,
            legend style={draw=none},
            xmin=0,
            xmax=30000,
            grid=major,
        ]
            \addplot table [
                x = training_step,
                y = reward,
            ] {results/default/LunarLander-v2/lr0.0.csv};
            \addlegendentry{MuZero};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/hybrid/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^g + l^c$};

            \addplot table [
                x = training_step,
                y = reward,
            ] {results/pretrained/LunarLander-v2/lr1.0.csv};
            \addlegendentry{$l^g + l^c$ (pretrained)};
        \end{axis}
    \end{tikzpicture}
    \caption{Total episode reward comparisson of agents using both the reconstruction function loss ($l^g$) as well as the consistency loss term ($l^c$) simultaneously as a pretrained and non-pretrained variant in the CartPole-v1 and LunarLander-v2 environments, averaged across 32 and 25 runs, respectively.}
    \label{fig:pretrained_results}
\end{figure}
When using self-supervised pretraining (see figure \ref{fig:pretrained_results}), training progresses very rapidly as soon as the goal is introduced. In the LunarLander-v2 environment, a mean total reward of $0$ is reached in roughly half the amount of training steps that are required by the non-pretrained agent. However, at later stages of training, the advantage fades, and, in the case of CartPole-v1, agents using self-supervised pretraining perform significantly worse than agents starting with randomly initialized networks.

The fully trained agents are compared in table \ref{tab:results_table}.
\begin{table}[ht]
    \centering
    \begin{tabular}{|l||c|c|}
        \hline
        & CartPole-v1 & LunarLander-v2 \\
        \hline \hline
        MuZero & $281.42 \pm 162.48$ & $-34.86 \pm 92.87$ \\
        \hline
        $l^g$ & $375.85 \pm 149.38$ & $42.67 \pm 132.59$\\
        \hline
        $l^c$ & $296.45 \pm 174.55$ & $32.17 \pm 145.90$ \\
        \hline
        $l^g + l^c$ & $\mathbf{410.59 \pm 130.71}$ & $100.46 \pm 123.82$ \\
        \hline
        $l^g + l^c$ (pretrained) & $335.72 \pm 162.49$ & $\mathbf{104.99 \pm 116.67}$ \\
        \hline
    \end{tabular}
    \caption{Comparisson of the default MuZero algorithm and the modifications described in this thesis on the CartPole-v1 and LunarLander-v2 environments. The terms $l^g$ and $l^c$ stands for the addition of a reconstruction or consistency loss, respectively. The results show the mean and standard deviation of the total reward for the final $500$ training steps across all test runs.}
    \label{tab:results_table}
\end{table}