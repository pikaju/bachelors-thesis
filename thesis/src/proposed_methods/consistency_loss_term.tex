\subsection{Consistency Loss Term}
We additionally propose a simple loss term for MuZero's loss equation, which we call the \textit{consistency} loss, and that does not require another function to be introduced into the system. The name originates from the possible inconsistencies in embedded state representations after each application of the dynamics function. The algorithm is completely unconstrained in choosing a different data format, so to speak, for each simulation step $k$.

Say, as an example, we have two subsequent observations $o_t$ and $o_{t+1}$, between which action $a_t$ was performed. We can create their corresponding internal state representations with the help of the representation function $h_\theta$:
\begin{equation*}
    \begin{array}{cc}
        s^0_t = h_\theta(o_1, ..., o_t), &
        s^0_{t+1} = h_\theta(o_1, ..., o_t, o_{t+1})
    \end{array}
\end{equation*}
By applying the dynamics function $g_\theta$ to $s_t^0$ as well as action $a_t$, we receive another state representation $s_t^1$ that is intuitively supposed to reflect the environment at timestep $t+1$, much like $s_{t+1}^0$. However, so long as both state representations allow for reward and value predictions, MuZero does not require them to match, or even be similar. This pattern persists with every iteration of $g_\theta$, as is apparent in Figure \ref{fig:consistency_loss}. To clarify further, imagine a constructed but theoretically legitimate example in which state vector $s^0_{t+1}$ uses only the first half of its dimensions, whereas $s^1_t$ uses only the second half.
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[node distance=1.25]
        \node (ot) {$o_t$};
        \node [left = of ot] (otm1) {$\dots$};
        \node [right = of ot] (otp1) {$o_{t+1}$};
        \node [right = of otp1] (otp2) {$o_{t+2}$};
        \node [right = of otp2] (otp3) {$o_{t+3}$};
        \node [right = of otp3] (otp4) {$\dots$};
        \draw [->, dashed] (otm1) -- (ot);
        \draw [->, dashed] (ot) -- (otp1);
        \draw [->, dashed] (otp1) -- (otp2);
        \draw [->, dashed] (otp2) -- (otp3);
        \draw [->, dashed] (otp3) -- (otp4);

        \node [below = 1.75 of ot] (s0) {$s^0_t$};
        \node [below = 1.75 of otp1] (s1) {$s^1_t$};
        \node [below = 1.75 of otp2] (s2) {$s^2_t$};
        \node [below = 1.75 of otp3] (s3) {$s^3_t$};

        \draw [->] (ot) -- node [left] {$h_\theta$} (s0);
        \draw [->] (s0) -- node [below] {$g_\theta$} (s1);
        \draw [->] (s1) -- node [below] {$g_\theta$} (s2);
        \draw [->] (s2) -- node [below] {$g_\theta$} (s3);

        \node [above right = 0.1 of s1] (nots1) {$s^0_{t+1}$};
        \node [above right = -0.25 of s1] {\Lightning};
        \node [above right = 0.1 of s2] (nots2) {$s^0_{t+2}$};
        \node [above right = -0.25 of s2] {\Lightning};
        \node [above right = 0.1 of s3] (nots3) {$s^0_{t+3}$};
        \node [above right = -0.25 of s3] {\Lightning};

        \draw [->] (otp1) edge [bend right] node [left] {$h_\theta$} (nots1);
        \draw [->] (otp2) edge [bend right] node [left] {$h_\theta$} (nots2);
        \draw [->] (otp3) edge [bend right] node [left] {$h_\theta$} (nots3);
    \end{tikzpicture}
    \caption{Visualization of the discrepancy between state outputs of the dynamics function $g_\theta$ and representation function $h_\theta$.}
    \label{fig:consistency_loss}
\end{figure}

While the existence of multiple state formats may not be inherently destructive, and has been suggested to provide some benefits for the agent's understanding of the environment, we believe it can cause several problems. The most obvious of which is the need for the dynamics and prediction functions to learn how to use each of the different representations for the same estimates. If instead, $h_\theta$ and $g_\theta$ agree on a unified state format, this problem can be avoided. A more subtle but potentially more significant issue becomes apparent when we inspect MuZero's learning process. The functions are unrolled for $K$ timesteps across a given trajectory, and losses are computed for each timestep $k \in \{0, ..., K\}$. This means that $g_\theta$ and $f_\theta$ are trained on any state format that is produced by $g_\theta$ after up to $K$ iterations. For any further iterations, accuracy may degenerate. Depending on the search policy, it is not unlikely for the planning depth to become larger than $K$. In fact, the hyperparameters used for the original MuZero experiments have a small $K=5$ unroll depth for training, while at the same time using $50$ or even $800$ simulations for tree construction during planning, which can easily result in branches that are longer than $K$. By enforcing a consistent state representation, we may be able to mitigate the degeneration of performance for long planning branches.

We implement our change by continuously adjusting $\theta$ so that output $s^k_t$ of the dynamics function is close to output $s^0_{t+k}$ of the representation function. For example, we can perform gradient descent on the mean squared error between the state vectors. Mathematically, we express the consistency loss as $l^c(s^0_{t+k}, s^k_t)$, leading to an overall loss of
\begin{equation*}
    l_t(\theta) = \sum^K_{k=0} \left(l^r(u_{t+k}, r_t^k) + l^v(z_{t+k}, v_t^k) + l^p(\pi_{t+k}, \policy_t^k) + l^c(s^0_{t+k}, s^k_t) + c||\theta||^2\right).
\end{equation*}
Note that we treat the first parameter of $l^c$, $s^0_{t+k}$, as a constant, meaning the loss is not propagated directly to the representation function. Doing so would encourage $h_\theta$ and $g_\theta$ to always produce the same state outputs, regardless of the input. As a bonus, by only propagating the error through $s_t^k$ instead, the model is forced to maintain relevant information to predict subsequent states even in environments with sparse rewards, similar to the reconstruction loss from our previous proposal.