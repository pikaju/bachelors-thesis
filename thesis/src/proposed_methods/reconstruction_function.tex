\newcommand{\reconstruction}{h^{-1}}

\subsection{Reconstruction Function}
For our first proposed change, we introduce an additional $\theta$-parameterized function, which we call the \textit{reconstruction} function $\reconstruction_\theta$. Whereas the representation function $h_\theta$ maps observations to internal states, the reconstruction function shall perform the inverse operation, that is, mapping internal states to real observations, thereby performing a generative task. Notably, since we are using function approximation, reconstructed observations are unlikely to be perfectly accurate. Moreover, in the probable case that the embedded states store less information than the real observations, it becomes theoretically impossible for $\reconstruction_\theta$ to restore what has been discarded by the representation function. We call $\hat{o}^k_t = \reconstruction_\theta(s^k_t)$ the reconstructed observation for the embedded state $s^k_t$, meaning it is an estimate of the real observation $o_{t+k}$ (see Figure \ref{fig:reconstruction_function}), given that the action sequences given to the environment and the dynamics function are the same.
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[node distance=1.25]
        \node (ot) {$o_t$};
        \node [left = of ot] (otm1) {$\dots$};
        \node [right = of ot] (otp1) {$o_{t+1}$};
        \node [right = of otp1] (otp2) {$o_{t+2}$};
        \node [right = of otp2] (otp3) {$o_{t+3}$};
        \node [right = of otp3] (otp4) {$\dots$};
        \draw [->, dashed] (otm1) -- (ot);
        \draw [->, dashed] (ot) -- (otp1);
        \draw [->, dashed] (otp1) -- (otp2);
        \draw [->, dashed] (otp2) -- (otp3);
        \draw [->, dashed] (otp3) -- (otp4);

        \node [below = 1.75 of ot] (s0) {$s^0_t$};
        \node [below = 1.75 of otp1] (s1) {$s^1_t$};
        \node [below = 1.75 of otp2] (s2) {$s^2_t$};
        \node [below = 1.75 of otp3] (s3) {$s^3_t$};
        \draw [->] (ot) -- node [left] {$h_\theta$} (s0);
        \draw [->] (s0) -- node [below] {$g_\theta$} (s1);
        \draw [->] (s1) -- node [below] {$g_\theta$} (s2);
        \draw [->] (s2) -- node [below] {$g_\theta$} (s3);

        \node [below right = 0.1 of ot] (notot) {$\hat{o}^0_t$};
        \node [below right = 0.1 of otp1] (nototp1) {$\hat{o}^1_t$};
        \node [below right = 0.1 of otp2] (nototp2) {$\hat{o}^2_t$};
        \node [below right = 0.1 of otp3] (nototp3) {$\hat{o}^3_t$};
        \node [below right = -0.2 of ot] {\Lightning};
        \node [below right = -0.2 of otp1] {\Lightning};
        \node [below right = -0.2 of otp2] {\Lightning};
        \node [below right = -0.2 of otp3] {\Lightning};

        \draw [->] (s0) edge [bend right] node [right] {$\reconstruction_\theta$} (notot);
        \draw [->] (s1) edge [bend right] node [right] {$\reconstruction_\theta$} (nototp1);
        \draw [->] (s2) edge [bend right] node [right] {$\reconstruction_\theta$} (nototp2);
        \draw [->] (s3) edge [bend right] node [right] {$\reconstruction_\theta$} (nototp3);
    \end{tikzpicture}
    \caption{The reconstruction function $\reconstruction_\theta$ being used to predict future observations $o_{t+k}$ from internal states $s^k$ with the help of the representation function $h_\theta$ as well as the dynamics function $g_\theta$.}
    \label{fig:reconstruction_function}
\end{figure}

The reconstruction function is trained with the help of another loss term $l^g$ added to the default MuZero loss equation
\begin{equation*}
    l_t(\theta) = \sum^K_{k=0} \left(l^r(u_{t+k}, r_t^k) + l^v(z_{t+k}, v_t^k) + l^p(\pi_{t+k}, \policy_t^k) + l^g(o_{t+k}, \hat{o}^k_t) + c||\theta||^2\right).
\end{equation*}
This loss term shall be smaller the better the model becomes at estimating observations, for example by determining the mean squared error between the real and the reconstructed observation. Notably, the error gradients must be propagated further through the dynamics function $g_\theta$, and eventually the representation function $h_\theta$. This means $h_\theta$ and $g_\theta$ are incentivized to maintain information that is useful for observation reconstruction.

Note that, so far, we have not specified any use cases for these reconstructed observations. We do not incorporate observation reconstruction in MuZero's planning process, and, in fact, the reconstruction function $\reconstruction_\theta$ may be discarded once the training process is complete. We are only concerned with the effects of the gradients for $l^g$ on the representation function $h_\theta$ and dynamics function $g_\theta$. To understand why, consider a MuZero agent navigating an environment with sparse rewards. It will observe many state transitions based on its actions that could reveal the potentially complex inner workings of the environment. Unless it is actually given rewards, however, it will skip out on gaining any insights, as the model's only goal is reward and value prediction. Even worse, the agent may be subject to \textit{catastrophic forgetting} of what has already been learned, as it is only being trained with a reward target of $0$ and small value targets. The reconstruction loss term $l^g$ shall counteract these issues by propagating its error gradients such that the representation function and dynamics function are forced, so to speak, to comprehend the environment beyond the rewards it supplies. Reconstructed observations are not meant to be accurate and their error gradients must not overpower the reward and value prediction. Instead, they should act merely as a guide to stabilize and accelerate learning.

An additional benefit is the ability to pretrain an agent in a self-supervised fashion. That is, the agent can explore an environment without being given any rewards (or any goal) in order to learn about its mechanics and develop a world model. This model can then be specialized to different goals within the same environment. The process is comparable to a child discovering a new task in an environment it is already familiar with, giving it an advantage by not having to learn from scratch.
